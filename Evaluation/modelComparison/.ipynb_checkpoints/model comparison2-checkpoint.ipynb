{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import join\n",
    "import pickle\n",
    "import sys\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisPath = os.getcwd()\n",
    "os.chdir(\"../../code\")\n",
    "projectDir = os.getcwd()\n",
    "from evalUtils import plotModelHist, UrgentVRoutne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reportBinaryScores(yTrueUrgent, yPredProbUrgent, v=0):\n",
    "    yPredUrgent = yPredProbUrgent.round().astype(np.int)\n",
    "    tn, fp, fn, tp = confusion_matrix(yTrueUrgent.astype(np.float), \n",
    "                                      yPredUrgent).ravel()\n",
    "    tpr = tp/(tp + fn)\n",
    "    tnr = tn/(tn + fp)\n",
    "    fpr = fp/(fp + tn)\n",
    "    fnr = fn/(fn + tp)\n",
    "    plr = tpr/fpr #positive likelihood ratio\n",
    "    nlr = fnr/tnr # negative likelihood ratio\n",
    "    acc = accuracy_score(yTrueUrgent, \n",
    "                         yPredUrgent)\n",
    "    if v:\n",
    "        print('\\t accuracy: {0:.3g}'.format(acc))\n",
    "        print(\"\\t sensitivity {0:.3g}\".format(tpr))\n",
    "        print(\"\\t specificity {0:.3g}\".format(tnr))\n",
    "        print(\"\\t positive likelihood ratio {0:.3g}\".format(plr))\n",
    "        print(\"\\t negative likelihood ratio {0:.3g}\".format(nlr))\n",
    "        print(\"\\n\")\n",
    "    return acc, tpr, tnr, plr, nlr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTestPath = r\"D:\\Projects\\OCT-Image-Classification\\PreprocessedData\\preprocessedForCNN\\224x224\\targetData_(224, 224, 3)_test.npy\"\n",
    "yTrueTest = np.load(yTestPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsPath = r\"D:\\Projects\\OCT-Image-Classification\\modelOutput\\compareModels2\"\n",
    "InceptionPath = r\"InceptionV3_dataAug_False2019-06-01_20_41_default\"\n",
    "ResNet50Path = r\"ResNet50_dataAug_False2019-06-01_18_31_default\" \n",
    "VGG16Path = r\"VGG16_dataAug_False2019-05-24_13_58_default\" \n",
    "#XceptionPath = r\"Xception_dataAug_False2019-05-24_21_22_default\"\n",
    "#r\"VGG16_dataAug_False2019-05-17_14_19_defaultVGG16_4L\" \n",
    "#VGGPath5 = r\"VGG16_dataAug_False2019-05-17_1_4_noGAP_2L\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirList = [VGG16Path, ResNet50Path, InceptionPath, ] #XceptionPath\n",
    "modelNames = [\"VGG16\", \"ResNet50\", \"InceptionV3\", ] #\"Xception\"\n",
    "modelPred = {}\n",
    "for i, (expDir, modelName) in enumerate(zip(dirList, modelNames)):\n",
    "    print(i, expDir)\n",
    "    expPath = join(modelsPath, expDir)\n",
    "    assert(os.path.isdir(expPath))\n",
    "    '''\n",
    "    modelPath = join(expPath, \"{}.hdf5\".format(modelName))\n",
    "    try:\n",
    "        model = load_model(modelPath)\n",
    "        model.summary()\n",
    "    except:\n",
    "        print('cannot load model')\n",
    "    '''\n",
    "    histPath = join(expPath, \"{}_History.csv\".format(modelName)) \n",
    "    modelHist = pd.read_csv(histPath, index_col=0)\n",
    "    plotModelHist(modelHist)\n",
    "    yTestPred = np.load(join(expPath, \"yTestPred.npy\"))\n",
    "    modelPred[modelName] = yTestPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classMap = {\n",
    "    \"NORMAL\": 0,\n",
    "    \"DRUSEN\": 1,\n",
    "    \"CNV\": 2,\n",
    "    \"DME\": 3}\n",
    "\n",
    "\n",
    "yTrue1Hot = to_categorical(yTrueTest)\n",
    "yTrueTestUrgent = UrgentVRoutne(yTrue1Hot, classMap).astype(np.int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AccListVGG16 = []\n",
    "SensListVGG16 = []\n",
    "SpecListVGG16 = []\n",
    "\n",
    "aucListVGG16 = []\n",
    "fprsListVGG16 = []\n",
    "tprsListVGG16 = []\n",
    "\n",
    "metricDict = {}\n",
    "for model_i in modelPred.keys():\n",
    "    print(model_i)\n",
    "    yTestPred = modelPred[model_i]\n",
    "    classAcc = accuracy_score(yTrueTest,\n",
    "                             yTestPred.argmax(axis=1))\n",
    "    print('\\t accuracy: {0:.3g}'.format(classAcc))\n",
    "    yTestPredUrgent = UrgentVRoutne(yTestPred, classMap)\n",
    "    print()\n",
    "    print('\\t binary (urgent vs non-urgent)')\n",
    "    scores = reportBinaryScores(yTrueTestUrgent, yTestPredUrgent, v=1)\n",
    "    acc, tpr, tnr, plr, nlr = scores\n",
    "    \n",
    "    fprs, tprs, _ = roc_curve(yTrueTestUrgent, yTestPredUrgent)\n",
    "    aucUrgent = auc(fprs, tprs)\n",
    "    \n",
    "    metricDict[model_i] = {}\n",
    "    metricDict[model_i][\"acc\"] = acc\n",
    "    metricDict[model_i][\"tpr\"] = tpr\n",
    "    metricDict[model_i][\"tnr\"] = tnr\n",
    "    metricDict[model_i][\"aucUrgent\"] = aucUrgent\n",
    "    metricDict[model_i][\"fprs\"] = fprs\n",
    "    metricDict[model_i][\"tprs\"] = tprs\n",
    "    \n",
    "    AccListVGG16.append(acc)\n",
    "    SensListVGG16.append(tpr)\n",
    "    SpecListVGG16.append(tnr)\n",
    "    \n",
    "    aucListVGG16.append(aucUrgent)\n",
    "    fprsListVGG16.append(fprs)\n",
    "    tprsListVGG16.append(tprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricsDF = pd.DataFrame(index=modelPred.keys(), columns=['acc', 'tpr', 'tnr', 'auc'])\n",
    "metricsDF[\"acc\"] = AccListVGG16\n",
    "metricsDF[\"tpr\"] = SensListVGG16\n",
    "metricsDF[\"tnr\"] = SpecListVGG16\n",
    "metricsDF[\"auc\"] = aucListVGG16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reportedAcc = 0.934\n",
    "reportedSens = 0.966\n",
    "reportedSpec = 0.94\n",
    "reportedAuc = 0.988\n",
    "\n",
    "reportedMetrics = [reportedAcc, \n",
    "                   reportedSens, \n",
    "                   reportedSpec,\n",
    "                   reportedAuc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opacity = 0.8\n",
    "error_config = {'ecolor': '0.3'}\n",
    "n_groups = 4\n",
    "nModels = len(modelPred.keys()) + 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.12\n",
    "colors = ['green', 'orange', 'blue', 'cyan']\n",
    "for i, (vgg_i, c) in enumerate(zip(modelPred.keys(), colors)):\n",
    "    \n",
    "    xpos = index - nModels*bar_width/2 + (i+1)*bar_width\n",
    "    rectsi = ax.bar(xpos, \n",
    "                    metricsDF.loc[vgg_i], bar_width,\n",
    "                    alpha=opacity, color=c,\n",
    "                    label=vgg_i)\n",
    "\n",
    "xpos = index - nModels*bar_width/2 + (i+2)*bar_width\n",
    "rects2 = ax.bar(xpos, reportedMetrics, bar_width,\n",
    "                alpha=opacity, color='r', \n",
    "                label='Baseline')\n",
    "\n",
    "#ax.set_xlabel('Group')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance')\n",
    "ax.set_xticks(index + bar_width / 2)\n",
    "ax.set_xticklabels(('Accuracy', 'Sensitivity', 'Specificity', 'AUC'))\n",
    "ax.set_ylim([0.0, 1.3])\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open()\n",
    "modelPred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
